{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b09d6f7f",
   "metadata": {},
   "source": [
    "# Fashion-MNIST Classification with an LDA Head\n",
    "This notebook trains a small convolutional encoder with a linear discriminant analysis (LDA) head on Fashion-MNIST, then visualises the learned embedding space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f89626",
   "metadata": {},
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80764dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from src.lda import LDAHead, LDALoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd667682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device =', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc74c579",
   "metadata": {},
   "source": [
    "### Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dbdd16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfm = transforms.ToTensor()\n",
    "train_ds = datasets.FashionMNIST(root='./data', train=True, transform=tfm, download=True)\n",
    "test_ds  = datasets.FashionMNIST(root='./data', train=False, transform=tfm, download=True)\n",
    "train_ld = DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_ld  = DataLoader(test_ds,  batch_size=1024, shuffle=False, num_workers=2, pin_memory=True)\n",
    "len(train_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaab3fc",
   "metadata": {},
   "source": [
    "### Model: encoder + LDA head (on-the-fly stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a641b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 256), nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 64), nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, dim),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class DeepLDA(nn.Module):\n",
    "    def __init__(self, C, D):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(D)\n",
    "        self.head = LDAHead(C, D)\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.head(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc29e0",
   "metadata": {},
   "source": [
    "### Train & Eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8039b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] train loss=-14.8614 acc=0.7711 | test acc=0.8300\n",
      "[02] train loss=-16.1347 acc=0.8559 | test acc=0.8574\n",
      "[03] train loss=-16.4379 acc=0.8727 | test acc=0.8654\n",
      "[04] train loss=-16.6159 acc=0.8816 | test acc=0.8670\n",
      "[05] train loss=-16.7528 acc=0.8885 | test acc=0.8746\n",
      "[06] train loss=-16.8628 acc=0.8941 | test acc=0.8766\n",
      "[07] train loss=-16.9395 acc=0.8982 | test acc=0.8800\n",
      "[08] train loss=-17.0027 acc=0.9026 | test acc=0.8788\n",
      "[09] train loss=-17.0557 acc=0.9047 | test acc=0.8818\n",
      "[10] train loss=-17.1113 acc=0.9086 | test acc=0.8807\n",
      "[11] train loss=-17.1489 acc=0.9113 | test acc=0.8851\n",
      "[12] train loss=-17.1944 acc=0.9147 | test acc=0.8886\n",
      "[13] train loss=-17.2362 acc=0.9177 | test acc=0.8882\n",
      "[14] train loss=-17.2516 acc=0.9189 | test acc=0.8893\n",
      "[15] train loss=-17.3022 acc=0.9228 | test acc=0.8879\n",
      "[16] train loss=-17.3224 acc=0.9256 | test acc=0.8893\n",
      "[17] train loss=-17.3514 acc=0.9262 | test acc=0.8909\n",
      "[18] train loss=-17.3747 acc=0.9294 | test acc=0.8917\n",
      "[19] train loss=-17.4066 acc=0.9314 | test acc=0.8919\n",
      "[20] train loss=-17.4470 acc=0.9348 | test acc=0.8912\n",
      "[21] train loss=-17.4596 acc=0.9358 | test acc=0.8958\n",
      "[22] train loss=-17.4778 acc=0.9375 | test acc=0.8949\n",
      "[23] train loss=-17.4921 acc=0.9386 | test acc=0.8917\n",
      "[24] train loss=-17.5207 acc=0.9407 | test acc=0.8935\n",
      "[25] train loss=-17.5389 acc=0.9418 | test acc=0.8943\n",
      "[26] train loss=-17.5670 acc=0.9440 | test acc=0.8992\n",
      "[27] train loss=-17.5766 acc=0.9441 | test acc=0.8963\n",
      "[28] train loss=-17.5974 acc=0.9466 | test acc=0.8950\n",
      "[29] train loss=-17.6149 acc=0.9476 | test acc=0.8949\n",
      "[30] train loss=-17.6174 acc=0.9482 | test acc=0.8897\n",
      "[31] train loss=-17.6495 acc=0.9507 | test acc=0.8917\n",
      "[32] train loss=-17.6648 acc=0.9516 | test acc=0.8932\n",
      "[33] train loss=-17.6847 acc=0.9531 | test acc=0.8942\n",
      "[34] train loss=-17.7036 acc=0.9535 | test acc=0.8943\n",
      "[35] train loss=-17.7097 acc=0.9544 | test acc=0.8943\n",
      "[36] train loss=-17.7224 acc=0.9550 | test acc=0.8958\n",
      "[37] train loss=-17.7405 acc=0.9577 | test acc=0.8973\n",
      "[38] train loss=-17.7584 acc=0.9575 | test acc=0.8930\n",
      "[39] train loss=-17.7789 acc=0.9595 | test acc=0.8933\n",
      "[40] train loss=-17.7736 acc=0.9587 | test acc=0.8976\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ok = tot = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)  # EMA stats\n",
    "        ok += (logits.argmax(1) == y).sum().item()\n",
    "        tot += y.size(0)\n",
    "    return ok / tot\n",
    "\n",
    "model = DeepLDA(C=10, D=9).to(device)\n",
    "opt = torch.optim.Adam(model.encoder.parameters())\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = nn.NLLLoss()\n",
    "loss_fn = LDALoss()\n",
    "\n",
    "for epoch in range(1, 41):\n",
    "    model.train()\n",
    "    loss_sum = acc_sum = n_sum = 0\n",
    "    for x, y in train_ld:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)        # uses batch stats + EMA update\n",
    "        loss = loss_fn(logits, y)\n",
    "        opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            acc_sum += (pred == y).sum().item()\n",
    "            n_sum += y.size(0)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "    tr_acc = acc_sum / n_sum\n",
    "    te_acc = evaluate(model, test_ld)\n",
    "    print(f\"[{epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
