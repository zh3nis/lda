{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d143aca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3 — Deep LDA: end-to-end likelihood training -> collapse\n",
    "# Encoder: 2-layer MLP (ReLU, 32 hidden) to 2-D embeddings; joint ML with Adam.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "rng = np.random.default_rng(1)\n",
    "#torch.manual_seed(1)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Synthetic data from LDA (Eq. (1)-(2))\n",
    "# ----------------------------\n",
    "C, d_in, n = 3, 4, 300          # input dim 4 (nonlinear projection to 2D by encoder)\n",
    "d_z = 2                          # embedding dim (for visualization)\n",
    "pi_true  = np.array([1/3, 1/3, 1/3])\n",
    "mus_true = np.array([[-2.0, -0.5,  0.5,  0.2],\n",
    "                     [ 2.0,  0.3, -0.2, -0.5],\n",
    "                     [-0.7, 2.4,  0.1,  0.3]], dtype=float)\n",
    "Sigma_true = np.array([[1.0, 0.35, 0.0, 0.0],\n",
    "                       [0.35, 1.1, 0.0, 0.0],\n",
    "                       [0.0, 0.0, 0.8, 0.2],\n",
    "                       [0.0, 0.0, 0.2, 0.9]], dtype=float)\n",
    "\n",
    "ys = rng.choice(C, size=n, p=pi_true)\n",
    "X  = np.vstack([rng.multivariate_normal(mus_true[c], Sigma_true) for c in ys]).astype(np.float32)\n",
    "\n",
    "# train/test split (to illustrate poor generalization)\n",
    "perm = rng.permutation(n)\n",
    "tr = perm[: n//2]; te = perm[n//2 :]\n",
    "Xtr, ytr = X[tr], ys[tr]\n",
    "Xte, yte = X[te], ys[te]\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Encoder: 2-layer MLP with ReLU, 32 hidden units -> R^2 (Fig. 3 setup)\n",
    "# ----------------------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_in, d_z, width=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, width),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(width, d_z),\n",
    "        )\n",
    "    def forward(self, x):  # x: [B,d_in]\n",
    "        return self.net(x)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) LDA head params (jointly learned): π, {μ_c}, Σ\n",
    "#    Unconstrained reparam: π=softmax(α); Σ = L L^T with log-diagonal (Adam)\n",
    "# ----------------------------\n",
    "class LDAHead(nn.Module):\n",
    "    def __init__(self, C, d_z):\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.zeros(C))           # logits -> π\n",
    "        self.mu    = nn.Parameter(torch.randn(C, d_z)*0.5)  # class means in z-space\n",
    "        self.L_free   = nn.Parameter(torch.zeros(d_z, d_z)) # strictly lower\n",
    "        self.log_diag = nn.Parameter(torch.zeros(d_z))      # diag(L) = exp(log_diag)\n",
    "\n",
    "    def cholesky(self):\n",
    "        L = torch.tril(self.L_free).clone()\n",
    "        idx = torch.arange(L.shape[0])\n",
    "        L[idx, idx] = torch.exp(self.log_diag)\n",
    "        return L\n",
    "\n",
    "    def logpdf_shared(self, z):\n",
    "        # z: [B,d_z]; returns [B,C] log N(z | μ_c, Σ)\n",
    "        B, d = z.shape\n",
    "        C = self.mu.shape[0]\n",
    "        L = self.cholesky()\n",
    "        invL = torch.inverse(L)                             # fine for d=2\n",
    "        logdet = 2.0 * torch.log(torch.diagonal(L)).sum()\n",
    "        diff = z[:, None, :] - self.mu[None, :, :]          # [B,C,d]\n",
    "        v = diff @ invL.T                                   # [B,C,d]\n",
    "        quad = (v**2).sum(dim=2)                            # [B,C]\n",
    "        return -0.5*(d*np.log(2*np.pi) + logdet + quad)\n",
    "\n",
    "    def logits(self, z):\n",
    "        # LDA log-likelihood term for each class + log prior (for NLL on (x,y))\n",
    "        log_pi = F.log_softmax(self.alpha, dim=0)\n",
    "        return self.logpdf_shared(z) + log_pi               # [B,C]\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Deep LDA model: f_ψ(x) + LDA head; optimize average log-likelihood (Eq. (4))\n",
    "# ----------------------------\n",
    "device = \"cpu\"\n",
    "enc = Encoder(d_in, d_z, width=32).to(device)\n",
    "head = LDAHead(C, d_z).to(device)\n",
    "\n",
    "Xtr_t = torch.tensor(Xtr, dtype=torch.float32, device=device)\n",
    "ytr_t = torch.tensor(ytr, dtype=torch.long, device=device)\n",
    "Xte_t = torch.tensor(Xte, dtype=torch.float32, device=device)\n",
    "yte_t = torch.tensor(yte, dtype=torch.long, device=device)\n",
    "\n",
    "opt = torch.optim.Adam(list(enc.parameters()) + list(head.parameters()), lr=3e-3)  # Adam, end-to-end\n",
    "# End-to-end ML objective:\n",
    "def nll_on_batch(x, y):\n",
    "    z = enc(x)\n",
    "    lp = head.logits(z)                                    # log p(y|z) up to constant (Eq. (4))\n",
    "    return -(lp[torch.arange(x.size(0)), y].mean())\n",
    "\n",
    "# simple training loop\n",
    "enc.train(); head.train()\n",
    "for t in range(2000):\n",
    "    loss = nll_on_batch(Xtr_t, ytr_t)\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if (t+1) % 400 == 0:\n",
    "        with torch.no_grad():\n",
    "            ztr = enc(Xtr_t)\n",
    "            lp_tr = head.logits(ztr).softmax(dim=1)\n",
    "            acc_tr = (lp_tr.argmax(1) == ytr_t).float().mean().item()\n",
    "            zte = enc(Xte_t)\n",
    "            lp_te = head.logits(zte).softmax(dim=1)\n",
    "            acc_te = (lp_te.argmax(1) == yte_t).float().mean().item()\n",
    "        print(f\"step {t+1:4d} | nll={loss.item():.4f} | acc_tr={acc_tr:.3f} | acc_te={acc_te:.3f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Visualize embeddings z = f_ψ(x) after likelihood training (collapse)\n",
    "# ----------------------------\n",
    "enc.eval(); head.eval()\n",
    "with torch.no_grad():\n",
    "    z_all = enc(torch.tensor(X, dtype=torch.float32, device=device)).cpu().numpy()\n",
    "    mu_ml = head.mu.detach().cpu().numpy()\n",
    "    L_ml  = head.cholesky().detach().cpu().numpy()\n",
    "    Sigma_ml = L_ml @ L_ml.T\n",
    "    logdet = np.log(np.linalg.det(Sigma_ml) + 1e-30)\n",
    "\n",
    "print(\"Shared Σ (learned) ~ near-singular?  log|Σ| =\", logdet)\n",
    "\n",
    "# Color-blind friendly colors; markers with black edges\n",
    "OKABE_ITO = [\"#0072B2\", \"#D55E00\", \"#009E73\"]\n",
    "colors = OKABE_ITO[:C]\n",
    "markers = [\"o\", \"s\", \"^\"]\n",
    "\n",
    "def cov_ellipse(mean, cov, k=2.15, **kwargs):  # ~90% mass in 2D\n",
    "    vals, vecs = np.linalg.eigh(cov)\n",
    "    order = vals.argsort()[::-1]\n",
    "    vals, vecs = vals[order], vecs[:, order]\n",
    "    width, height = 2 * k * np.sqrt(np.maximum(vals, 1e-12))\n",
    "    angle = np.degrees(np.arctan2(vecs[1, 0], vecs[0, 0]))\n",
    "    return Ellipse(xy=mean, width=width, height=height, angle=angle, **kwargs)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 14, \"axes.labelsize\": 16, \"legend.fontsize\": 13,\n",
    "    \"xtick.labelsize\": 13, \"ytick.labelsize\": 13\n",
    "})\n",
    "fig, ax = plt.subplots(figsize=(7.2, 5.8), constrained_layout=True)\n",
    "\n",
    "# scatter embeddings by class (labels start from 1)\n",
    "for c in range(C):\n",
    "    idx = (ys == c)\n",
    "    ax.scatter(z_all[idx,0], z_all[idx,1], s=22, c=colors[c], marker=markers[c],\n",
    "               edgecolors=\"black\", linewidths=0.5, alpha=0.9, label=f\"class {c+1}\")\n",
    "\n",
    "# learned class centers and (tiny) covariance ellipses (shared Σ for all)\n",
    "for c in range(C):\n",
    "    ax.scatter(mu_ml[c,0], mu_ml[c,1], s=140, marker=\"X\", c=colors[c],\n",
    "               edgecolors=\"black\", linewidths=1.0, zorder=5)\n",
    "    #e = cov_ellipse(mu_ml[c], Sigma_ml, k=2.15, edgecolor=\"black\", facecolor=\"none\",\n",
    "    #                lw=1.2, linestyle=\"--\", alpha=0.9)\n",
    "    #ax.add_patch(e)\n",
    "\n",
    "ax.set_xlabel(r\"$z_1$\")\n",
    "ax.set_ylabel(r\"$z_2$\")\n",
    "ax.legend(frameon=False, ncol=3, loc=\"upper right\")\n",
    "#ax.axis(\"equal\")\n",
    "pad = 0\n",
    "xmin, xmax = z_all[:,0].min()-pad, z_all[:,0].max()+pad\n",
    "ymin, ymax = z_all[:,1].min()-pad, z_all[:,1].max()+pad\n",
    "#ax.set_xlim(xmin, xmax); ax.set_ylim(ymin, ymax)\n",
    "#ax.set_xlim(.14, .16)\n",
    "#ax.set_ylim(.56, .59)\n",
    "ax.grid(True, linewidth=0.4, alpha=0.35)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
