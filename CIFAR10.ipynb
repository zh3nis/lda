{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0802afa3",
   "metadata": {},
   "source": [
    "# CIFAR-10 Classification with an LDA Head\n",
    "This notebook trains a lightweight convolutional encoder with an LDA head on CIFAR-10, alongside a softmax baseline, then visualises the learned embedding spaces side by side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c1fc5d",
   "metadata": {},
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf11f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from src.lda import TrainableLDAHead, DiagTrainableLDAHead, FullCovLDAHead, DNLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab011bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device =', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7881674",
   "metadata": {},
   "source": [
    "### Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a8e436e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2470, 0.2435, 0.2616)\n",
    "pin_memory = torch.cuda.is_available()\n",
    "\n",
    "train_tfm = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "test_tfm = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "train_ds = datasets.CIFAR10(root='./data', train=True, transform=train_tfm, download=True)\n",
    "test_ds  = datasets.CIFAR10(root='./data', train=False, transform=test_tfm, download=True)\n",
    "train_ld = DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=4, pin_memory=pin_memory)\n",
    "test_ld  = DataLoader(test_ds,  batch_size=1024, shuffle=False, num_workers=4, pin_memory=pin_memory)\n",
    "len(train_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0efcdea",
   "metadata": {},
   "source": [
    "### Model: encoder + heads (LDA + softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daebb93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.proj = nn.Linear(256, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.proj(x)\n",
    "\n",
    "class DeepLDA(nn.Module):\n",
    "    def __init__(self, C, D):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(D)\n",
    "        self.head = DiagTrainableLDAHead(C, D)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.head(z)\n",
    "\n",
    "class SoftmaxHead(nn.Module):\n",
    "    def __init__(self, D, C):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(D, C)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.linear(z)\n",
    "\n",
    "class DeepClassifier(nn.Module):\n",
    "    def __init__(self, C, D):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(D)\n",
    "        self.head = SoftmaxHead(D, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.head(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd8e3cd",
   "metadata": {},
   "source": [
    "### Train & Eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f80cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LDA 01] train loss=7.5954 acc=0.4139 | test acc=0.5519\n",
      "[LDA 02] train loss=5.1689 acc=0.6046 | test acc=0.5798\n",
      "[LDA 03] train loss=3.5886 acc=0.6736 | test acc=0.6528\n",
      "[LDA 04] train loss=2.1670 acc=0.7206 | test acc=0.6678\n",
      "[LDA 05] train loss=0.8214 acc=0.7448 | test acc=0.6623\n",
      "[LDA 06] train loss=-0.4681 acc=0.7676 | test acc=0.7404\n",
      "[LDA 07] train loss=-1.5934 acc=0.7821 | test acc=0.7273\n",
      "[LDA 08] train loss=-2.4113 acc=0.8038 | test acc=0.7666\n",
      "[LDA 09] train loss=-2.8413 acc=0.8267 | test acc=0.8054\n",
      "[LDA 10] train loss=-3.0198 acc=0.8443 | test acc=0.7767\n",
      "[LDA 11] train loss=-3.0967 acc=0.8573 | test acc=0.8176\n",
      "[LDA 12] train loss=-3.1517 acc=0.8699 | test acc=0.8137\n",
      "[LDA 13] train loss=-3.1787 acc=0.8767 | test acc=0.8147\n",
      "[LDA 14] train loss=-3.2080 acc=0.8845 | test acc=0.8447\n",
      "[LDA 15] train loss=-3.2317 acc=0.8930 | test acc=0.8459\n",
      "[LDA 16] train loss=-3.2432 acc=0.8962 | test acc=0.8383\n",
      "[LDA 17] train loss=-3.2562 acc=0.9010 | test acc=0.8446\n",
      "[LDA 18] train loss=-3.2727 acc=0.9067 | test acc=0.8617\n",
      "[LDA 19] train loss=-3.2820 acc=0.9099 | test acc=0.8341\n",
      "[LDA 20] train loss=-3.2886 acc=0.9115 | test acc=0.8541\n",
      "[LDA 21] train loss=-3.3033 acc=0.9170 | test acc=0.8698\n",
      "[LDA 22] train loss=-3.3113 acc=0.9215 | test acc=0.8579\n",
      "[LDA 23] train loss=-3.3227 acc=0.9250 | test acc=0.8669\n",
      "[LDA 24] train loss=-3.3305 acc=0.9275 | test acc=0.8685\n",
      "[LDA 25] train loss=-3.3340 acc=0.9275 | test acc=0.8722\n",
      "[LDA 26] train loss=-3.3483 acc=0.9335 | test acc=0.8795\n",
      "[LDA 27] train loss=-3.3546 acc=0.9350 | test acc=0.8757\n",
      "[LDA 28] train loss=-3.3640 acc=0.9401 | test acc=0.8678\n",
      "[LDA 29] train loss=-3.3727 acc=0.9429 | test acc=0.8627\n",
      "[LDA 30] train loss=-3.3770 acc=0.9433 | test acc=0.8797\n",
      "[LDA 31] train loss=-3.3796 acc=0.9456 | test acc=0.8479\n",
      "[LDA 32] train loss=-3.3962 acc=0.9512 | test acc=0.8863\n",
      "[LDA 33] train loss=-3.3997 acc=0.9525 | test acc=0.8657\n",
      "[LDA 34] train loss=-3.4015 acc=0.9532 | test acc=0.8862\n",
      "[LDA 35] train loss=-3.4076 acc=0.9545 | test acc=0.8806\n",
      "[LDA 36] train loss=-3.4138 acc=0.9576 | test acc=0.8728\n",
      "[LDA 37] train loss=-3.4174 acc=0.9591 | test acc=0.8779\n",
      "[LDA 38] train loss=-3.4266 acc=0.9623 | test acc=0.8961\n",
      "[LDA 39] train loss=-3.4314 acc=0.9629 | test acc=0.8842\n",
      "[LDA 40] train loss=-3.4392 acc=0.9651 | test acc=0.8703\n",
      "[LDA 41] train loss=-3.4403 acc=0.9670 | test acc=0.8859\n",
      "[LDA 42] train loss=-3.4476 acc=0.9690 | test acc=0.8821\n",
      "[LDA 43] train loss=-3.4521 acc=0.9707 | test acc=0.8894\n",
      "[LDA 44] train loss=-3.4571 acc=0.9728 | test acc=0.8983\n",
      "[LDA 45] train loss=-3.4568 acc=0.9715 | test acc=0.8940\n",
      "[LDA 46] train loss=-3.4590 acc=0.9735 | test acc=0.8870\n",
      "[LDA 47] train loss=-3.4653 acc=0.9745 | test acc=0.8954\n",
      "[LDA 48] train loss=-3.4676 acc=0.9755 | test acc=0.8983\n",
      "[LDA 49] train loss=-3.4696 acc=0.9757 | test acc=0.8887\n",
      "[LDA 50] train loss=-3.4787 acc=0.9789 | test acc=0.8967\n",
      "[LDA 51] train loss=-3.4771 acc=0.9787 | test acc=0.8978\n",
      "[LDA 52] train loss=-3.4862 acc=0.9815 | test acc=0.8862\n",
      "[LDA 53] train loss=-3.4837 acc=0.9807 | test acc=0.8929\n",
      "[LDA 54] train loss=-3.4865 acc=0.9815 | test acc=0.8975\n",
      "[LDA 55] train loss=-3.4906 acc=0.9826 | test acc=0.9011\n",
      "[LDA 56] train loss=-3.4954 acc=0.9843 | test acc=0.8972\n",
      "[LDA 57] train loss=-3.4969 acc=0.9853 | test acc=0.8929\n",
      "[LDA 58] train loss=-3.4970 acc=0.9838 | test acc=0.9005\n",
      "[LDA 59] train loss=-3.5001 acc=0.9845 | test acc=0.8999\n",
      "[LDA 60] train loss=-3.5064 acc=0.9869 | test acc=0.8874\n",
      "[LDA 61] train loss=-3.5062 acc=0.9869 | test acc=0.9022\n",
      "[LDA 62] train loss=-3.5076 acc=0.9869 | test acc=0.9052\n",
      "[LDA 63] train loss=-3.5102 acc=0.9882 | test acc=0.9022\n",
      "[LDA 64] train loss=-3.5123 acc=0.9884 | test acc=0.9025\n",
      "[LDA 65] train loss=-3.5102 acc=0.9881 | test acc=0.8981\n",
      "[LDA 66] train loss=-3.5136 acc=0.9887 | test acc=0.9041\n",
      "[LDA 67] train loss=-3.5150 acc=0.9892 | test acc=0.8987\n",
      "[LDA 68] train loss=-3.5212 acc=0.9909 | test acc=0.8996\n",
      "[LDA 69] train loss=-3.5233 acc=0.9909 | test acc=0.9034\n",
      "[LDA 70] train loss=-3.5235 acc=0.9905 | test acc=0.9015\n",
      "[LDA 71] train loss=-3.5223 acc=0.9908 | test acc=0.8965\n",
      "[LDA 72] train loss=-3.5229 acc=0.9906 | test acc=0.8949\n",
      "[LDA 73] train loss=-3.5250 acc=0.9909 | test acc=0.8956\n",
      "[LDA 74] train loss=-3.5261 acc=0.9916 | test acc=0.8990\n",
      "[LDA 75] train loss=-3.5281 acc=0.9917 | test acc=0.9010\n",
      "[LDA 76] train loss=-3.5289 acc=0.9921 | test acc=0.9006\n",
      "[LDA 77] train loss=-3.5319 acc=0.9924 | test acc=0.9004\n",
      "[LDA 78] train loss=-3.5296 acc=0.9922 | test acc=0.9038\n",
      "[LDA 79] train loss=-3.5321 acc=0.9929 | test acc=0.8903\n",
      "[LDA 80] train loss=-3.5349 acc=0.9936 | test acc=0.9005\n",
      "[LDA 81] train loss=-3.5340 acc=0.9925 | test acc=0.8995\n",
      "[LDA 82] train loss=-3.5370 acc=0.9941 | test acc=0.8931\n",
      "[LDA 83] train loss=-3.5376 acc=0.9943 | test acc=0.8894\n",
      "[LDA 84] train loss=-3.5363 acc=0.9932 | test acc=0.8986\n",
      "[LDA 85] train loss=-3.5387 acc=0.9942 | test acc=0.8973\n",
      "[LDA 86] train loss=-3.5414 acc=0.9944 | test acc=0.9074\n",
      "[LDA 87] train loss=-3.5426 acc=0.9951 | test acc=0.9063\n",
      "[LDA 88] train loss=-3.5429 acc=0.9951 | test acc=0.8993\n",
      "[LDA 89] train loss=-3.5427 acc=0.9946 | test acc=0.9067\n",
      "[LDA 90] train loss=-3.5431 acc=0.9944 | test acc=0.9041\n",
      "[LDA 91] train loss=-3.5446 acc=0.9951 | test acc=0.9043\n",
      "[LDA 92] train loss=-3.5468 acc=0.9953 | test acc=0.8963\n",
      "[LDA 93] train loss=-3.5485 acc=0.9957 | test acc=0.8883\n",
      "[LDA 94] train loss=-3.5481 acc=0.9956 | test acc=0.9017\n",
      "[LDA 95] train loss=-3.5465 acc=0.9951 | test acc=0.9080\n",
      "[LDA 96] train loss=-3.5491 acc=0.9957 | test acc=0.9014\n",
      "[LDA 97] train loss=-3.5511 acc=0.9959 | test acc=0.9074\n",
      "[LDA 98] train loss=-3.5509 acc=0.9957 | test acc=0.9050\n",
      "[LDA 99] train loss=-3.5520 acc=0.9963 | test acc=0.9011\n",
      "[LDA 100] train loss=-3.5503 acc=0.9958 | test acc=0.9040\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ok = tot = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        ok += (logits.argmax(1) == y).sum().item()\n",
    "        tot += y.size(0)\n",
    "    return ok / tot\n",
    "\n",
    "lda_model = DeepLDA(C=10, D=9).to(device)\n",
    "lda_opt = torch.optim.Adam(lda_model.parameters())\n",
    "lda_loss_fn = DNLLLoss(lambda_reg=.01)\n",
    "\n",
    "lda_train_acc = []\n",
    "lda_test_acc = []\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    lda_model.train()\n",
    "    loss_sum = acc_sum = n_sum = 0\n",
    "    for x, y in train_ld:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = lda_model(x)\n",
    "        loss = lda_loss_fn(logits, y)\n",
    "        lda_opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        lda_opt.step()\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            acc_sum += (pred == y).sum().item()\n",
    "            n_sum += y.size(0)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "    tr_acc = acc_sum / n_sum\n",
    "    te_acc = evaluate(lda_model, test_ld)\n",
    "    lda_train_acc.append(tr_acc)\n",
    "    lda_test_acc.append(te_acc)\n",
    "    print(f\"[LDA {epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b23943",
   "metadata": {},
   "source": [
    "### Softmax head baseline\n",
    "Train a second model with the same encoder but a standard softmax classifier for a side-by-side comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999aec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_model = DeepClassifier(C=10, D=9).to(device)\n",
    "softmax_opt = torch.optim.Adam(softmax_model.parameters())\n",
    "softmax_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "softmax_train_acc = []\n",
    "softmax_test_acc = []\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    softmax_model.train()\n",
    "    loss_sum = acc_sum = n_sum = 0\n",
    "    for x, y in train_ld:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = softmax_model(x)\n",
    "        loss = softmax_loss_fn(logits, y)\n",
    "        softmax_opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        softmax_opt.step()\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            acc_sum += (pred == y).sum().item()\n",
    "            n_sum += y.size(0)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "    tr_acc = acc_sum / n_sum\n",
    "    te_acc = evaluate(softmax_model, test_ld)\n",
    "    softmax_train_acc.append(tr_acc)\n",
    "    softmax_test_acc.append(te_acc)\n",
    "    print(f\"[Softmax {epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_confidence_hist(model, loader, out_path, title=None):\n",
    "    model.eval()\n",
    "    conf_list, pred_list, label_list = [], [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        conf, pred = probs.max(1)\n",
    "        conf_list.append(conf.cpu())\n",
    "        pred_list.append(pred.cpu())\n",
    "        label_list.append(y.cpu())\n",
    "\n",
    "    conf = torch.cat(conf_list).numpy()\n",
    "    pred = torch.cat(pred_list)\n",
    "    labels = torch.cat(label_list)\n",
    "    acc = (pred == labels).float().mean().item()\n",
    "    avg_conf = conf.mean().item()\n",
    "\n",
    "    bins = np.linspace(0.0, 1.0, 21)\n",
    "    weights = np.ones_like(conf) / conf.size\n",
    "\n",
    "    plt.figure(figsize=(4.5, 4.5))\n",
    "    plt.hist(conf, bins=bins, weights=weights, color='blue', edgecolor='black')\n",
    "    plt.axvline(avg_conf, color='gray', linestyle='--', linewidth=3)\n",
    "    plt.axvline(acc, color='gray', linestyle='--', linewidth=3)\n",
    "    plt.text(avg_conf, 0.95 * plt.gca().get_ylim()[1], 'Avg confidence',\n",
    "             rotation=90, va='top', ha='center')\n",
    "    plt.text(acc, 0.95 * plt.gca().get_ylim()[1], 'Accuracy',\n",
    "             rotation=90, va='top', ha='center')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('% of Samples')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=600)\n",
    "\n",
    "plot_confidence_hist(lda_model, test_ld, 'plots/cifar10_lda_confidence_hist.png', title='LDA')\n",
    "plot_confidence_hist(softmax_model, test_ld, 'plots/cifar10_softmax_confidence_hist.png', title='Softmax')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d812f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_embeddings(model, loader, batches=10):\n",
    "    model.eval()\n",
    "    embeds, labels = [], []\n",
    "    for i, (x, y) in enumerate(loader):   # use train_ld if you prefer\n",
    "        x = x.to(device)\n",
    "        z = model.encoder(x).cpu()\n",
    "        embeds.append(z)\n",
    "        labels.append(y)\n",
    "        if i >= batches - 1:   # 10 batches â‰ˆ10k points; raise/lower to taste\n",
    "            break\n",
    "\n",
    "    z = torch.cat(embeds)\n",
    "    y = torch.cat(labels)\n",
    "\n",
    "    # 2D projection (PCA)\n",
    "    z0 = z - z.mean(0, keepdim=True)\n",
    "    _, _, V = torch.pca_lowrank(z0, q=2)\n",
    "    z2 = z0 @ V[:, :2]\n",
    "    return z2, y\n",
    "\n",
    "lda_z2, lda_y = collect_embeddings(lda_model, train_ld)\n",
    "softmax_z2, softmax_y = collect_embeddings(softmax_model, train_ld)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 14, \"axes.labelsize\": 16, \"legend.fontsize\": 13,\n",
    "    \"xtick.labelsize\": 13, \"ytick.labelsize\": 13\n",
    "})\n",
    "\n",
    "palette = plt.cm.tab10.colors\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "plots = [\n",
    "    (axes[0], lda_z2, lda_y, \"Deep LDA embeddings\"),\n",
    "    (axes[1], softmax_z2, softmax_y, \"Softmax head embeddings\"),\n",
    "]\n",
    "\n",
    "for ax, z2, y, title in plots:\n",
    "    for c in range(10):\n",
    "        idx = y == c\n",
    "        ax.scatter(z2[idx, 0], z2[idx, 1], s=8, alpha=0.6, color=palette[c], label=train_ds.classes[c])\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, bbox_to_anchor=(0.5, -0.02), loc=\"lower center\", ncol=5)\n",
    "plt.tight_layout(rect=(0, 0.07, 1, 1))\n",
    "plt.savefig('plots/cifar10_lda_vs_softmax_embeddings.png', dpi=600)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dzz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
