{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0802afa3",
   "metadata": {},
   "source": [
    "# CIFAR-10 Classification with an LDA Head\n",
    "This notebook trains a lightweight convolutional encoder with an LDA head on CIFAR-10, alongside a softmax baseline, then visualises the learned embedding spaces side by side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c1fc5d",
   "metadata": {},
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf11f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from src.lda import TrainableLDAHead, AllLogSigmoidLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab011bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device =', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7881674",
   "metadata": {},
   "source": [
    "### Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a8e436e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2470, 0.2435, 0.2616)\n",
    "pin_memory = torch.cuda.is_available()\n",
    "\n",
    "train_tfm = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "test_tfm = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "train_ds = datasets.CIFAR10(root='./data', train=True, transform=train_tfm, download=True)\n",
    "test_ds  = datasets.CIFAR10(root='./data', train=False, transform=test_tfm, download=True)\n",
    "train_ld = DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=4, pin_memory=pin_memory)\n",
    "test_ld  = DataLoader(test_ds,  batch_size=1024, shuffle=False, num_workers=4, pin_memory=pin_memory)\n",
    "len(train_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0efcdea",
   "metadata": {},
   "source": [
    "### Model: encoder + heads (LDA + softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daebb93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.proj = nn.Linear(256, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.proj(x)\n",
    "\n",
    "class DeepLDA(nn.Module):\n",
    "    def __init__(self, C, D):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(D)\n",
    "        self.head = TrainableLDAHead(C, D)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.head(z)\n",
    "\n",
    "class SoftmaxHead(nn.Module):\n",
    "    def __init__(self, D, C):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(D, C)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.linear(z)\n",
    "\n",
    "class DeepClassifier(nn.Module):\n",
    "    def __init__(self, C, D):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(D)\n",
    "        self.head = SoftmaxHead(D, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.head(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd8e3cd",
   "metadata": {},
   "source": [
    "### Train & Eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f80cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LDA 01] train loss=3.1647 acc=0.1978 | test acc=0.2889\n",
      "[LDA 02] train loss=2.7048 acc=0.3530 | test acc=0.4169\n",
      "[LDA 03] train loss=2.2337 acc=0.5260 | test acc=0.5499\n",
      "[LDA 04] train loss=1.7411 acc=0.6589 | test acc=0.6625\n",
      "[LDA 05] train loss=1.3983 acc=0.7244 | test acc=0.5694\n",
      "[LDA 06] train loss=1.1813 acc=0.7685 | test acc=0.7433\n",
      "[LDA 07] train loss=1.0243 acc=0.8030 | test acc=0.7634\n",
      "[LDA 08] train loss=0.9064 acc=0.8251 | test acc=0.8131\n",
      "[LDA 09] train loss=0.8276 acc=0.8432 | test acc=0.8197\n",
      "[LDA 10] train loss=0.7455 acc=0.8598 | test acc=0.7966\n",
      "[LDA 11] train loss=0.6944 acc=0.8684 | test acc=0.8341\n",
      "[LDA 12] train loss=0.6440 acc=0.8787 | test acc=0.8197\n",
      "[LDA 13] train loss=0.5947 acc=0.8876 | test acc=0.8355\n",
      "[LDA 14] train loss=0.5593 acc=0.8947 | test acc=0.8275\n",
      "[LDA 15] train loss=0.5275 acc=0.9016 | test acc=0.8400\n",
      "[LDA 16] train loss=0.4981 acc=0.9067 | test acc=0.8538\n",
      "[LDA 17] train loss=0.4709 acc=0.9124 | test acc=0.8296\n",
      "[LDA 18] train loss=0.4433 acc=0.9186 | test acc=0.8612\n",
      "[LDA 19] train loss=0.4227 acc=0.9215 | test acc=0.8465\n",
      "[LDA 20] train loss=0.4002 acc=0.9255 | test acc=0.8737\n",
      "[LDA 21] train loss=0.3810 acc=0.9308 | test acc=0.8634\n",
      "[LDA 22] train loss=0.3523 acc=0.9359 | test acc=0.8760\n",
      "[LDA 23] train loss=0.3453 acc=0.9379 | test acc=0.8799\n",
      "[LDA 24] train loss=0.3237 acc=0.9418 | test acc=0.8424\n",
      "[LDA 25] train loss=0.3102 acc=0.9450 | test acc=0.8771\n",
      "[LDA 26] train loss=0.2844 acc=0.9491 | test acc=0.8775\n",
      "[LDA 27] train loss=0.2798 acc=0.9502 | test acc=0.8885\n",
      "[LDA 28] train loss=0.2649 acc=0.9538 | test acc=0.8838\n",
      "[LDA 29] train loss=0.2554 acc=0.9544 | test acc=0.8669\n",
      "[LDA 30] train loss=0.2466 acc=0.9576 | test acc=0.8780\n",
      "[LDA 31] train loss=0.2305 acc=0.9596 | test acc=0.8919\n",
      "[LDA 32] train loss=0.2176 acc=0.9628 | test acc=0.8835\n",
      "[LDA 33] train loss=0.2033 acc=0.9657 | test acc=0.8836\n",
      "[LDA 34] train loss=0.2093 acc=0.9638 | test acc=0.8794\n",
      "[LDA 35] train loss=0.2022 acc=0.9660 | test acc=0.8718\n",
      "[LDA 36] train loss=0.1825 acc=0.9702 | test acc=0.8754\n",
      "[LDA 37] train loss=0.1787 acc=0.9706 | test acc=0.8768\n",
      "[LDA 38] train loss=0.1668 acc=0.9726 | test acc=0.8944\n",
      "[LDA 39] train loss=0.1668 acc=0.9731 | test acc=0.8791\n",
      "[LDA 40] train loss=0.1590 acc=0.9737 | test acc=0.8847\n",
      "[LDA 41] train loss=0.1551 acc=0.9751 | test acc=0.8870\n",
      "[LDA 42] train loss=0.1476 acc=0.9755 | test acc=0.8884\n",
      "[LDA 43] train loss=0.1441 acc=0.9769 | test acc=0.8883\n",
      "[LDA 44] train loss=0.1413 acc=0.9771 | test acc=0.8892\n",
      "[LDA 45] train loss=0.1263 acc=0.9800 | test acc=0.8927\n",
      "[LDA 46] train loss=0.1317 acc=0.9787 | test acc=0.8799\n",
      "[LDA 47] train loss=0.1201 acc=0.9807 | test acc=0.8897\n",
      "[LDA 48] train loss=0.1183 acc=0.9812 | test acc=0.8898\n",
      "[LDA 49] train loss=0.1227 acc=0.9796 | test acc=0.8834\n",
      "[LDA 50] train loss=0.1180 acc=0.9809 | test acc=0.8788\n",
      "[LDA 51] train loss=0.1164 acc=0.9812 | test acc=0.8842\n",
      "[LDA 52] train loss=0.1051 acc=0.9839 | test acc=0.8861\n",
      "[LDA 53] train loss=0.0975 acc=0.9851 | test acc=0.8928\n",
      "[LDA 54] train loss=0.1062 acc=0.9829 | test acc=0.8914\n",
      "[LDA 55] train loss=0.1003 acc=0.9847 | test acc=0.8938\n",
      "[LDA 56] train loss=0.1040 acc=0.9841 | test acc=0.8929\n",
      "[LDA 57] train loss=0.0884 acc=0.9868 | test acc=0.8886\n",
      "[LDA 58] train loss=0.0912 acc=0.9862 | test acc=0.8938\n",
      "[LDA 59] train loss=0.0864 acc=0.9870 | test acc=0.8867\n",
      "[LDA 60] train loss=0.0931 acc=0.9858 | test acc=0.8985\n",
      "[LDA 61] train loss=0.0946 acc=0.9860 | test acc=0.8775\n",
      "[LDA 62] train loss=0.0831 acc=0.9881 | test acc=0.8971\n",
      "[LDA 63] train loss=0.0897 acc=0.9860 | test acc=0.8939\n",
      "[LDA 64] train loss=0.0745 acc=0.9900 | test acc=0.8944\n",
      "[LDA 65] train loss=0.0759 acc=0.9890 | test acc=0.8954\n",
      "[LDA 66] train loss=0.0753 acc=0.9892 | test acc=0.8908\n",
      "[LDA 67] train loss=0.0878 acc=0.9868 | test acc=0.8929\n",
      "[LDA 68] train loss=0.0768 acc=0.9888 | test acc=0.8950\n",
      "[LDA 69] train loss=0.0695 acc=0.9901 | test acc=0.8938\n",
      "[LDA 70] train loss=0.0719 acc=0.9896 | test acc=0.8962\n",
      "[LDA 71] train loss=0.0666 acc=0.9907 | test acc=0.8959\n",
      "[LDA 72] train loss=0.0818 acc=0.9881 | test acc=0.8939\n",
      "[LDA 73] train loss=0.0715 acc=0.9900 | test acc=0.8985\n",
      "[LDA 74] train loss=0.0648 acc=0.9906 | test acc=0.8916\n",
      "[LDA 75] train loss=0.0591 acc=0.9918 | test acc=0.8983\n",
      "[LDA 76] train loss=0.0738 acc=0.9895 | test acc=0.8939\n",
      "[LDA 77] train loss=0.0639 acc=0.9909 | test acc=0.8938\n",
      "[LDA 78] train loss=0.0609 acc=0.9917 | test acc=0.9002\n",
      "[LDA 79] train loss=0.0622 acc=0.9914 | test acc=0.8907\n",
      "[LDA 80] train loss=0.0586 acc=0.9919 | test acc=0.9011\n",
      "[LDA 81] train loss=0.0590 acc=0.9924 | test acc=0.8887\n",
      "[LDA 82] train loss=0.0651 acc=0.9905 | test acc=0.8917\n",
      "[LDA 83] train loss=0.0635 acc=0.9911 | test acc=0.8957\n",
      "[LDA 84] train loss=0.0566 acc=0.9918 | test acc=0.8930\n",
      "[LDA 85] train loss=0.0636 acc=0.9914 | test acc=0.9043\n",
      "[LDA 86] train loss=0.0495 acc=0.9932 | test acc=0.8951\n",
      "[LDA 87] train loss=0.0537 acc=0.9925 | test acc=0.8878\n",
      "[LDA 88] train loss=0.0568 acc=0.9922 | test acc=0.8957\n",
      "[LDA 89] train loss=0.0608 acc=0.9911 | test acc=0.8987\n",
      "[LDA 90] train loss=0.0497 acc=0.9935 | test acc=0.8991\n",
      "[LDA 91] train loss=0.0578 acc=0.9919 | test acc=0.8894\n",
      "[LDA 92] train loss=0.0604 acc=0.9914 | test acc=0.9018\n",
      "[LDA 93] train loss=0.0543 acc=0.9928 | test acc=0.9022\n",
      "[LDA 94] train loss=0.0491 acc=0.9932 | test acc=0.9030\n",
      "[LDA 95] train loss=0.0408 acc=0.9947 | test acc=0.9020\n",
      "[LDA 96] train loss=0.0467 acc=0.9937 | test acc=0.8869\n",
      "[LDA 97] train loss=0.0449 acc=0.9941 | test acc=0.8972\n",
      "[LDA 98] train loss=0.0528 acc=0.9928 | test acc=0.9016\n",
      "[LDA 99] train loss=0.0501 acc=0.9930 | test acc=0.8938\n",
      "[LDA 100] train loss=0.0518 acc=0.9929 | test acc=0.9001\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ok = tot = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        ok += (logits.argmax(1) == y).sum().item()\n",
    "        tot += y.size(0)\n",
    "    return ok / tot\n",
    "\n",
    "lda_model = DeepLDA(C=10, D=9).to(device)\n",
    "lda_opt = torch.optim.Adam(lda_model.parameters())\n",
    "lda_loss_fn = AllLogSigmoidLoss()\n",
    "\n",
    "lda_train_acc = []\n",
    "lda_test_acc = []\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    lda_model.train()\n",
    "    loss_sum = acc_sum = n_sum = 0\n",
    "    for x, y in train_ld:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = lda_model(x)\n",
    "        loss = lda_loss_fn(logits, y)\n",
    "        lda_opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        lda_opt.step()\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            acc_sum += (pred == y).sum().item()\n",
    "            n_sum += y.size(0)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "    tr_acc = acc_sum / n_sum\n",
    "    te_acc = evaluate(lda_model, test_ld)\n",
    "    lda_train_acc.append(tr_acc)\n",
    "    lda_test_acc.append(te_acc)\n",
    "    print(f\"[LDA {epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b23943",
   "metadata": {},
   "source": [
    "### Softmax head baseline\n",
    "Train a second model with the same encoder but a standard softmax classifier for a side-by-side comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999aec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_model = DeepClassifier(C=10, D=9).to(device)\n",
    "softmax_opt = torch.optim.Adam(softmax_model.parameters())\n",
    "softmax_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "softmax_train_acc = []\n",
    "softmax_test_acc = []\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    softmax_model.train()\n",
    "    loss_sum = acc_sum = n_sum = 0\n",
    "    for x, y in train_ld:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = softmax_model(x)\n",
    "        loss = softmax_loss_fn(logits, y)\n",
    "        softmax_opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        softmax_opt.step()\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            acc_sum += (pred == y).sum().item()\n",
    "            n_sum += y.size(0)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "    tr_acc = acc_sum / n_sum\n",
    "    te_acc = evaluate(softmax_model, test_ld)\n",
    "    softmax_train_acc.append(tr_acc)\n",
    "    softmax_test_acc.append(te_acc)\n",
    "    print(f\"[Softmax {epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d812f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_embeddings(model, loader, batches=10):\n",
    "    model.eval()\n",
    "    embeds, labels = [], []\n",
    "    for i, (x, y) in enumerate(loader):   # use train_ld if you prefer\n",
    "        x = x.to(device)\n",
    "        z = model.encoder(x).cpu()\n",
    "        embeds.append(z)\n",
    "        labels.append(y)\n",
    "        if i >= batches - 1:   # 10 batches â‰ˆ10k points; raise/lower to taste\n",
    "            break\n",
    "\n",
    "    z = torch.cat(embeds)\n",
    "    y = torch.cat(labels)\n",
    "\n",
    "    # 2D projection (PCA)\n",
    "    z0 = z - z.mean(0, keepdim=True)\n",
    "    _, _, V = torch.pca_lowrank(z0, q=2)\n",
    "    z2 = z0 @ V[:, :2]\n",
    "    return z2, y\n",
    "\n",
    "lda_z2, lda_y = collect_embeddings(lda_model, train_ld)\n",
    "softmax_z2, softmax_y = collect_embeddings(softmax_model, train_ld)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 14, \"axes.labelsize\": 16, \"legend.fontsize\": 13,\n",
    "    \"xtick.labelsize\": 13, \"ytick.labelsize\": 13\n",
    "})\n",
    "\n",
    "palette = plt.cm.tab10.colors\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "plots = [\n",
    "    (axes[0], lda_z2, lda_y, \"Deep LDA embeddings\"),\n",
    "    (axes[1], softmax_z2, softmax_y, \"Softmax head embeddings\"),\n",
    "]\n",
    "\n",
    "for ax, z2, y, title in plots:\n",
    "    for c in range(10):\n",
    "        idx = y == c\n",
    "        ax.scatter(z2[idx, 0], z2[idx, 1], s=8, alpha=0.6, color=palette[c], label=train_ds.classes[c])\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, bbox_to_anchor=(0.5, -0.02), loc=\"lower center\", ncol=5)\n",
    "plt.tight_layout(rect=(0, 0.07, 1, 1))\n",
    "plt.savefig('plots/cifar10_lda_vs_softmax_embeddings.png', dpi=600)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
