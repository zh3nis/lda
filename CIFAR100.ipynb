{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CIFAR-100 Classification with an LDA Head\n",
        "This notebook trains a lightweight convolutional encoder with a linear discriminant analysis (LDA) head on CIFAR-100, then visualises the learned embedding space.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fbf11f8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from src.lda import LDAHead\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ab011bd1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device = cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device =', device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7881674",
      "metadata": {},
      "source": [
        "### Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4a8e436e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 10000)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean = (0.5071, 0.4867, 0.4408)\n",
        "std = (0.2675, 0.2565, 0.2761)\n",
        "pin_memory = torch.cuda.is_available()\n",
        "\n",
        "train_tfm = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "test_tfm = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "train_ds = datasets.CIFAR100(root='./data', train=True, transform=train_tfm, download=True)\n",
        "test_ds  = datasets.CIFAR100(root='./data', train=False, transform=test_tfm, download=True)\n",
        "train_ld = DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=4, pin_memory=pin_memory)\n",
        "test_ld  = DataLoader(test_ds,  batch_size=1024, shuffle=False, num_workers=4, pin_memory=pin_memory)\n",
        "len(train_ds), len(test_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0efcdea",
      "metadata": {},
      "source": [
        "### Model: encoder + LDA head (on-the-fly stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "daebb93f",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "        )\n",
        "        self.proj = nn.Linear(256, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.proj(x)\n",
        "\n",
        "class DeepLDA(nn.Module):\n",
        "    def __init__(self, C, D):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(D)\n",
        "        self.head = LDAHead(C, D)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        return self.head(z)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcd8e3cd",
      "metadata": {},
      "source": [
        "### Train & Eval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "32f80cf5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[01] train loss=4.1493 acc=0.0785 | test acc=0.1407\n",
            "[02] train loss=-4.7983 acc=0.1719 | test acc=0.1625\n",
            "[03] train loss=-13.4931 acc=0.2398 | test acc=0.2156\n",
            "[04] train loss=-21.9968 acc=0.3027 | test acc=0.2784\n",
            "[05] train loss=-30.2758 acc=0.3563 | test acc=0.3272\n",
            "[06] train loss=-38.2826 acc=0.3992 | test acc=0.3695\n",
            "[07] train loss=-46.0033 acc=0.4426 | test acc=0.4061\n",
            "[08] train loss=-53.3566 acc=0.4804 | test acc=0.4509\n",
            "[09] train loss=-60.3492 acc=0.5105 | test acc=0.4592\n",
            "[10] train loss=-66.9237 acc=0.5406 | test acc=0.4704\n",
            "[11] train loss=-72.9923 acc=0.5587 | test acc=0.5199\n",
            "[12] train loss=-78.5586 acc=0.5824 | test acc=0.5288\n",
            "[13] train loss=-83.5665 acc=0.6023 | test acc=0.5029\n",
            "[14] train loss=-88.1345 acc=0.6188 | test acc=0.5420\n",
            "[15] train loss=-91.9183 acc=0.6310 | test acc=0.5655\n",
            "[16] train loss=-95.2519 acc=0.6460 | test acc=0.5699\n",
            "[17] train loss=-98.1293 acc=0.6616 | test acc=0.5796\n",
            "[18] train loss=-100.4656 acc=0.6748 | test acc=0.5924\n",
            "[19] train loss=-102.4967 acc=0.6870 | test acc=0.5868\n",
            "[20] train loss=-104.0742 acc=0.6970 | test acc=0.6079\n",
            "[21] train loss=-105.7430 acc=0.7096 | test acc=0.6159\n",
            "[22] train loss=-107.0351 acc=0.7175 | test acc=0.6158\n",
            "[23] train loss=-108.0824 acc=0.7264 | test acc=0.6278\n",
            "[24] train loss=-109.2339 acc=0.7348 | test acc=0.6325\n",
            "[25] train loss=-110.1856 acc=0.7439 | test acc=0.6217\n",
            "[26] train loss=-111.0216 acc=0.7497 | test acc=0.6327\n",
            "[27] train loss=-112.0579 acc=0.7587 | test acc=0.6289\n",
            "[28] train loss=-112.8483 acc=0.7638 | test acc=0.6374\n",
            "[29] train loss=-113.7969 acc=0.7705 | test acc=0.6396\n",
            "[30] train loss=-114.6290 acc=0.7765 | test acc=0.6452\n",
            "[31] train loss=-115.2411 acc=0.7802 | test acc=0.6429\n",
            "[32] train loss=-116.1031 acc=0.7879 | test acc=0.6432\n",
            "[33] train loss=-116.6821 acc=0.7906 | test acc=0.6432\n",
            "[34] train loss=-117.5978 acc=0.7962 | test acc=0.6449\n",
            "[35] train loss=-118.1456 acc=0.7999 | test acc=0.6502\n",
            "[36] train loss=-118.8315 acc=0.8043 | test acc=0.6451\n",
            "[37] train loss=-119.3792 acc=0.8077 | test acc=0.6446\n",
            "[38] train loss=-120.0600 acc=0.8135 | test acc=0.6395\n",
            "[39] train loss=-120.7599 acc=0.8171 | test acc=0.6460\n",
            "[40] train loss=-121.3236 acc=0.8195 | test acc=0.6544\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    ok = tot = 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        ok += (logits.argmax(1) == y).sum().item()\n",
        "        tot += y.size(0)\n",
        "    return ok / tot\n",
        "\n",
        "model = DeepLDA(C=100, D=99).to(device)\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "#loss_fn = nn.CrossEntropyLoss()\n",
        "loss_fn = nn.NLLLoss()\n",
        "\n",
        "for epoch in range(1, 41):\n",
        "    model.train()\n",
        "    loss_sum = acc_sum = n_sum = 0\n",
        "    for x, y in train_ld:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = loss_fn(logits, y)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        with torch.no_grad():\n",
        "            pred = logits.argmax(1)\n",
        "            acc_sum += (pred == y).sum().item()\n",
        "            n_sum += y.size(0)\n",
        "            loss_sum += loss.item() * y.size(0)\n",
        "    tr_acc = acc_sum / n_sum\n",
        "    te_acc = evaluate(model, test_ld)\n",
        "    print(f\"[{epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
