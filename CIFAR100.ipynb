{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0802afa3",
   "metadata": {},
   "source": [
    "# CIFAR-100 Classification with an LDA Head\n",
    "This notebook trains a lightweight convolutional encoder with a linear discriminant analysis (LDA) head on CIFAR-100, then visualises the learned embedding space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c1fc5d",
   "metadata": {},
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf11f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from src.lda import TrainableLDAHead, LogisticLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab011bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device =', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7881674",
   "metadata": {},
   "source": [
    "### Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (0.5071, 0.4867, 0.4408)\n",
    "std = (0.2675, 0.2565, 0.2761)\n",
    "pin_memory = torch.cuda.is_available()\n",
    "\n",
    "train_tfm = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "test_tfm = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "train_ds = datasets.CIFAR100(root='./data', train=True, transform=train_tfm, download=True)\n",
    "test_ds  = datasets.CIFAR100(root='./data', train=False, transform=test_tfm, download=True)\n",
    "train_ld = DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=4, pin_memory=pin_memory)\n",
    "test_ld  = DataLoader(test_ds,  batch_size=1024, shuffle=False, num_workers=4, pin_memory=pin_memory)\n",
    "len(train_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0efcdea",
   "metadata": {},
   "source": [
    "### Model: encoder + LDA head (on-the-fly stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daebb93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.proj = nn.Linear(256, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.proj(x)\n",
    "\n",
    "class DeepLDA(nn.Module):\n",
    "    def __init__(self, C, D):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(D)\n",
    "        self.head = TrainableLDAHead(C, D)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.head(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd8e3cd",
   "metadata": {},
   "source": [
    "### Train & Eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f80cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ok = tot = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        ok += (logits.argmax(1) == y).sum().item()\n",
    "        tot += y.size(0)\n",
    "    return ok / tot\n",
    "\n",
    "model = DeepLDA(C=100, D=99).to(device)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = nn.NLLLoss()\n",
    "#loss_fn = LDALoss()\n",
    "#loss_fn = nn.PoissonNLLLoss()\n",
    "loss_fn = LogisticLoss()\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    model.train()\n",
    "    loss_sum = acc_sum = n_sum = 0\n",
    "    for x, y in train_ld:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        #y_oh = 100 * F.one_hot(y, num_classes=100).to(logits.dtype)  # shape [batch,100]\n",
    "        loss = loss_fn(logits, y)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            acc_sum += (pred == y).sum().item()\n",
    "            n_sum += y.size(0)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "    tr_acc = acc_sum / n_sum\n",
    "    te_acc = evaluate(model, test_ld)\n",
    "    train_acc.append(tr_acc)\n",
    "    test_acc.append(te_acc)\n",
    "    print(f\"[{epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1de92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(train_acc) + 1)\n",
    "plt.figure(figsize=(6, 4))\n",
    "#plt.plot(epochs, train_acc, label='Train accuracy')\n",
    "plt.plot(epochs, test_acc, label='Test accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('CIFAR-100 accuracy during training')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff97720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "save_path = 'cifar100_model.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': opt.state_dict(),\n",
    "}, save_path)\n",
    "print(f'Saved model checkpoint to {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869debcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load saved model\n",
    "# checkpoint = torch.load('cifar100_model.pth', map_location=device)\n",
    "# model = DeepLDA(C=100, D=99).to(device)\n",
    "# opt = torch.optim.Adam(model.parameters())\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# model.eval()\n",
    "# print('Loaded checkpoint from cifar100_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64e7d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class means on training data and plot pairwise distances\n",
    "@torch.no_grad()\n",
    "def compute_class_means(model, loader, num_classes=100):\n",
    "    model.eval()\n",
    "    embedding_dim = model.head.D\n",
    "    sums = torch.zeros(num_classes, embedding_dim, device=device)\n",
    "    counts = torch.zeros(num_classes, device=device)\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        z = model.encoder(x)\n",
    "        sums.index_add_(0, y, z)\n",
    "        counts.index_add_(0, y, torch.ones_like(y, dtype=torch.float))\n",
    "    return (sums / counts.unsqueeze(1)).cpu()\n",
    "\n",
    "class_means = compute_class_means(model, train_ld)\n",
    "pairwise = torch.pdist(class_means, p=2).numpy()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(pairwise, bins=30, color='steelblue', edgecolor='black')\n",
    "plt.xlabel('Pairwise L2 distance between class means')\n",
    "plt.ylabel('Count')\n",
    "plt.title('CIFAR-100 class mean distances')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea49e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few examples from the two closest classes\n",
    "import numpy as np\n",
    "\n",
    "# Find closest pair using class means from the previous cell\n",
    "with torch.no_grad():\n",
    "    dist_mat = torch.cdist(class_means, class_means)\n",
    "    dist_mat.fill_diagonal_(float('inf'))\n",
    "    row_min, row_argmin = dist_mat.min(dim=1)\n",
    "    cls_a = row_min.argmin().item()\n",
    "    cls_b = row_argmin[cls_a].item()\n",
    "\n",
    "cls_names = train_ds.classes\n",
    "print(f\"Closest classes: {cls_a} ({cls_names[cls_a]}) and {cls_b} ({cls_names[cls_b]}), distance={dist_mat[cls_a, cls_b]:.4f}\")\n",
    "\n",
    "# Collect a few raw images (no augmentation) for each class from the training set\n",
    "n_per_class = 4\n",
    "indices_a = [i for i, t in enumerate(train_ds.targets) if t == cls_a][:n_per_class]\n",
    "indices_b = [i for i, t in enumerate(train_ds.targets) if t == cls_b][:n_per_class]\n",
    "\n",
    "fig, axes = plt.subplots(2, n_per_class, figsize=(2 * n_per_class, 4))\n",
    "for j, idx in enumerate(indices_a):\n",
    "    axes[0, j].imshow(train_ds.data[idx])\n",
    "    axes[0, j].axis('off')\n",
    "    axes[0, j].set_title(f\"{cls_names[cls_a]}\")\n",
    "for j, idx in enumerate(indices_b):\n",
    "    axes[1, j].imshow(train_ds.data[idx])\n",
    "    axes[1, j].axis('off')\n",
    "    axes[1, j].set_title(f\"{cls_names[cls_b]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc8716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples from the 2nd closest pair of classes\n",
    "# Assumes class_means computed above\n",
    "with torch.no_grad():\n",
    "    dist_mat = torch.cdist(class_means, class_means)\n",
    "    dist_mat.fill_diagonal_(float('inf'))\n",
    "    i_idx, j_idx = torch.triu_indices(dist_mat.size(0), dist_mat.size(1), offset=1)\n",
    "    pair_dists = dist_mat[i_idx, j_idx]\n",
    "    if pair_dists.numel() < 2:\n",
    "        raise RuntimeError('Need at least two class pairs to find the 2nd closest pair.')\n",
    "    sorted_vals, sorted_idx = torch.sort(pair_dists)\n",
    "    second_idx = sorted_idx[1].item()\n",
    "    cls_a = i_idx[second_idx].item()\n",
    "    cls_b = j_idx[second_idx].item()\n",
    "    pair_dist = sorted_vals[1].item()\n",
    "\n",
    "cls_names = train_ds.classes\n",
    "print(f\"2nd closest classes: {cls_a} ({cls_names[cls_a]}) and {cls_b} ({cls_names[cls_b]}), distance={pair_dist:.4f}\")\n",
    "\n",
    "n_per_class = 4\n",
    "indices_a = [i for i, t in enumerate(train_ds.targets) if t == cls_a][:n_per_class]\n",
    "indices_b = [i for i, t in enumerate(train_ds.targets) if t == cls_b][:n_per_class]\n",
    "\n",
    "fig, axes = plt.subplots(2, n_per_class, figsize=(2 * n_per_class, 4))\n",
    "for j, idx in enumerate(indices_a):\n",
    "    axes[0, j].imshow(train_ds.data[idx])\n",
    "    axes[0, j].axis('off')\n",
    "    axes[0, j].set_title(f\"{cls_names[cls_a]}\")\n",
    "for j, idx in enumerate(indices_b):\n",
    "    axes[1, j].imshow(train_ds.data[idx])\n",
    "    axes[1, j].axis('off')\n",
    "    axes[1, j].set_title(f\"{cls_names[cls_b]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
