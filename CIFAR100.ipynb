{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CIFAR-100 Classification with an LDA Head\n",
        "This notebook trains a lightweight convolutional encoder with a linear discriminant analysis (LDA) head on CIFAR-100, then visualises the learned embedding space.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from src.lda import LDAHead\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device = cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device =', device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 10000)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean = (0.5071, 0.4867, 0.4408)\n",
        "std = (0.2675, 0.2565, 0.2761)\n",
        "pin_memory = torch.cuda.is_available()\n",
        "\n",
        "train_tfm = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "test_tfm = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "train_ds = datasets.CIFAR100(root='./data', train=True, transform=train_tfm, download=True)\n",
        "test_ds  = datasets.CIFAR100(root='./data', train=False, transform=test_tfm, download=True)\n",
        "train_ld = DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=4, pin_memory=pin_memory)\n",
        "test_ld  = DataLoader(test_ds,  batch_size=1024, shuffle=False, num_workers=4, pin_memory=pin_memory)\n",
        "len(train_ds), len(test_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model: encoder + LDA head (on-the-fly stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "        )\n",
        "        self.proj = nn.Linear(256, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.proj(x)\n",
        "\n",
        "class DeepLDA(nn.Module):\n",
        "    def __init__(self, C, D):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(D)\n",
        "        self.head = LDAHead(C, D)\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        z = self.encoder(x)\n",
        "        return self.head(z, y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train & Eval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[01] train loss=3.5344 acc=0.1600 | test acc=0.1966\n",
            "[02] train loss=2.7156 acc=0.3053 | test acc=0.2877\n",
            "[03] train loss=2.2901 acc=0.3960 | test acc=0.3189\n",
            "[04] train loss=2.0183 acc=0.4556 | test acc=0.3686\n",
            "[05] train loss=1.8206 acc=0.4982 | test acc=0.4114\n",
            "[06] train loss=1.6670 acc=0.5385 | test acc=0.4252\n",
            "[07] train loss=1.5449 acc=0.5689 | test acc=0.5105\n",
            "[08] train loss=1.4413 acc=0.5946 | test acc=0.5115\n",
            "[09] train loss=1.3477 acc=0.6168 | test acc=0.5269\n",
            "[10] train loss=1.2690 acc=0.6365 | test acc=0.5485\n",
            "[11] train loss=1.2124 acc=0.6530 | test acc=0.5144\n",
            "[12] train loss=1.1456 acc=0.6672 | test acc=0.5635\n",
            "[13] train loss=1.0907 acc=0.6838 | test acc=0.5686\n",
            "[14] train loss=1.0411 acc=0.6943 | test acc=0.5702\n",
            "[15] train loss=0.9928 acc=0.7072 | test acc=0.5855\n",
            "[16] train loss=0.9476 acc=0.7201 | test acc=0.5836\n",
            "[17] train loss=0.9045 acc=0.7316 | test acc=0.5850\n",
            "[18] train loss=0.8653 acc=0.7423 | test acc=0.5825\n",
            "[19] train loss=0.8237 acc=0.7535 | test acc=0.6008\n",
            "[20] train loss=0.7939 acc=0.7611 | test acc=0.6030\n",
            "[21] train loss=0.7612 acc=0.7704 | test acc=0.6119\n",
            "[22] train loss=0.7320 acc=0.7773 | test acc=0.6194\n",
            "[23] train loss=0.6999 acc=0.7860 | test acc=0.6267\n",
            "[24] train loss=0.6727 acc=0.7940 | test acc=0.6197\n",
            "[25] train loss=0.6456 acc=0.8008 | test acc=0.5986\n",
            "[26] train loss=0.6219 acc=0.8089 | test acc=0.6267\n",
            "[27] train loss=0.5896 acc=0.8176 | test acc=0.6228\n",
            "[28] train loss=0.5746 acc=0.8202 | test acc=0.6146\n",
            "[29] train loss=0.5424 acc=0.8320 | test acc=0.6226\n",
            "[30] train loss=0.5227 acc=0.8364 | test acc=0.6269\n",
            "[31] train loss=0.5093 acc=0.8395 | test acc=0.6374\n",
            "[32] train loss=0.4844 acc=0.8468 | test acc=0.6295\n",
            "[33] train loss=0.4657 acc=0.8530 | test acc=0.6281\n",
            "[34] train loss=0.4501 acc=0.8580 | test acc=0.6064\n",
            "[35] train loss=0.4342 acc=0.8610 | test acc=0.6194\n",
            "[36] train loss=0.4194 acc=0.8680 | test acc=0.6375\n",
            "[37] train loss=0.4035 acc=0.8709 | test acc=0.6331\n",
            "[38] train loss=0.3868 acc=0.8762 | test acc=0.6171\n",
            "[39] train loss=0.3730 acc=0.8813 | test acc=0.6356\n",
            "[40] train loss=0.3582 acc=0.8848 | test acc=0.6373\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    ok = tot = 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        ok += (logits.argmax(1) == y).sum().item()\n",
        "        tot += y.size(0)\n",
        "    return ok / tot\n",
        "\n",
        "model = DeepLDA(C=100, D=99).to(device)\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, 41):\n",
        "    model.train()\n",
        "    loss_sum = acc_sum = n_sum = 0\n",
        "    for x, y in train_ld:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x, y)\n",
        "        loss = loss_fn(logits, y)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        with torch.no_grad():\n",
        "            pred = logits.argmax(1)\n",
        "            acc_sum += (pred == y).sum().item()\n",
        "            n_sum += y.size(0)\n",
        "            loss_sum += loss.item() * y.size(0)\n",
        "    tr_acc = acc_sum / n_sum\n",
        "    te_acc = evaluate(model, test_ld)\n",
        "    print(f\"[{epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
