{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CIFAR-100 Classification with an LDA Head\n",
        "This notebook trains a lightweight convolutional encoder with a linear discriminant analysis (LDA) head on CIFAR-100, then visualises the learned embedding space.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from src.lda import LDAHead\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device = cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device =', device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 10000)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean = (0.5071, 0.4867, 0.4408)\n",
        "std = (0.2675, 0.2565, 0.2761)\n",
        "pin_memory = torch.cuda.is_available()\n",
        "\n",
        "train_tfm = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "test_tfm = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "train_ds = datasets.CIFAR100(root='./data', train=True, transform=train_tfm, download=True)\n",
        "test_ds  = datasets.CIFAR100(root='./data', train=False, transform=test_tfm, download=True)\n",
        "train_ld = DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=4, pin_memory=pin_memory)\n",
        "test_ld  = DataLoader(test_ds,  batch_size=1024, shuffle=False, num_workers=4, pin_memory=pin_memory)\n",
        "len(train_ds), len(test_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model: encoder + LDA head (on-the-fly stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "        )\n",
        "        self.proj = nn.Linear(256, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.proj(x)\n",
        "\n",
        "class DeepLDA(nn.Module):\n",
        "    def __init__(self, C, D):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(D)\n",
        "        self.head = LDAHead(C, D)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        return self.head(z)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train & Eval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[01] train loss=3.5312 acc=0.1560 | test acc=0.2247\n",
            "[02] train loss=2.7162 acc=0.3028 | test acc=0.2711\n",
            "[03] train loss=2.3094 acc=0.3909 | test acc=0.3392\n",
            "[04] train loss=2.0245 acc=0.4556 | test acc=0.4268\n",
            "[05] train loss=1.8295 acc=0.5013 | test acc=0.4286\n",
            "[06] train loss=1.6631 acc=0.5414 | test acc=0.4943\n",
            "[07] train loss=1.5494 acc=0.5668 | test acc=0.4921\n",
            "[08] train loss=1.4413 acc=0.5933 | test acc=0.5230\n",
            "[09] train loss=1.3555 acc=0.6140 | test acc=0.4951\n",
            "[10] train loss=1.2800 acc=0.6345 | test acc=0.5606\n",
            "[11] train loss=1.2046 acc=0.6526 | test acc=0.5093\n",
            "[12] train loss=1.1487 acc=0.6691 | test acc=0.5543\n",
            "[13] train loss=1.0922 acc=0.6828 | test acc=0.5647\n",
            "[14] train loss=1.0396 acc=0.6957 | test acc=0.5858\n",
            "[15] train loss=0.9915 acc=0.7091 | test acc=0.5905\n",
            "[16] train loss=0.9516 acc=0.7197 | test acc=0.6049\n",
            "[17] train loss=0.9122 acc=0.7308 | test acc=0.5694\n",
            "[18] train loss=0.8665 acc=0.7428 | test acc=0.5916\n",
            "[19] train loss=0.8304 acc=0.7517 | test acc=0.6026\n",
            "[20] train loss=0.7980 acc=0.7596 | test acc=0.6115\n",
            "[21] train loss=0.7638 acc=0.7697 | test acc=0.6169\n",
            "[22] train loss=0.7284 acc=0.7798 | test acc=0.6027\n",
            "[23] train loss=0.6996 acc=0.7877 | test acc=0.6215\n",
            "[24] train loss=0.6678 acc=0.7969 | test acc=0.6178\n",
            "[25] train loss=0.6455 acc=0.8006 | test acc=0.6000\n",
            "[26] train loss=0.6192 acc=0.8097 | test acc=0.6082\n",
            "[27] train loss=0.5935 acc=0.8171 | test acc=0.6128\n",
            "[28] train loss=0.5768 acc=0.8208 | test acc=0.6211\n",
            "[29] train loss=0.5468 acc=0.8311 | test acc=0.6116\n",
            "[30] train loss=0.5247 acc=0.8373 | test acc=0.6322\n",
            "[31] train loss=0.5052 acc=0.8439 | test acc=0.6219\n",
            "[32] train loss=0.4851 acc=0.8479 | test acc=0.6227\n",
            "[33] train loss=0.4746 acc=0.8509 | test acc=0.6259\n",
            "[34] train loss=0.4473 acc=0.8582 | test acc=0.6169\n",
            "[35] train loss=0.4360 acc=0.8621 | test acc=0.6259\n",
            "[36] train loss=0.4219 acc=0.8667 | test acc=0.6453\n",
            "[37] train loss=0.4018 acc=0.8710 | test acc=0.6357\n",
            "[38] train loss=0.3896 acc=0.8753 | test acc=0.6286\n",
            "[39] train loss=0.3795 acc=0.8791 | test acc=0.6259\n",
            "[40] train loss=0.3595 acc=0.8847 | test acc=0.6350\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    ok = tot = 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        ok += (logits.argmax(1) == y).sum().item()\n",
        "        tot += y.size(0)\n",
        "    return ok / tot\n",
        "\n",
        "model = DeepLDA(C=100, D=99).to(device)\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, 41):\n",
        "    model.train()\n",
        "    loss_sum = acc_sum = n_sum = 0\n",
        "    for x, y in train_ld:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = loss_fn(logits, y)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        with torch.no_grad():\n",
        "            pred = logits.argmax(1)\n",
        "            acc_sum += (pred == y).sum().item()\n",
        "            n_sum += y.size(0)\n",
        "            loss_sum += loss.item() * y.size(0)\n",
        "    tr_acc = acc_sum / n_sum\n",
        "    te_acc = evaluate(model, test_ld)\n",
        "    print(f\"[{epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
