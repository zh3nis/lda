{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification with a Softmax Head\n",
    "This notebook mirrors the LDA-based workflow, but uses a standard softmax classifier head on top of the encoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device =', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfm = transforms.ToTensor()\n",
    "train_ds = datasets.MNIST(root='./data', train=True, transform=tfm, download=True)\n",
    "test_ds  = datasets.MNIST(root='./data', train=False, transform=tfm, download=True)\n",
    "train_ld = DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_ld  = DataLoader(test_ds,  batch_size=1024, shuffle=False, num_workers=2, pin_memory=True)\n",
    "len(train_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: encoder + softmax head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28, 256), nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 64), nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class SoftmaxHead(nn.Module):\n",
    "    def __init__(self, D, C):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(D, C)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.linear(z)\n",
    "\n",
    "class DeepClassifier(nn.Module):\n",
    "    def __init__(self, C, D):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(D)\n",
    "        self.head = SoftmaxHead(D, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.head(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] train loss=0.6153 acc=0.8251 | test acc=0.9282\n",
      "[02] train loss=0.2075 acc=0.9422 | test acc=0.9517\n",
      "[03] train loss=0.1436 acc=0.9584 | test acc=0.9563\n",
      "[04] train loss=0.1133 acc=0.9667 | test acc=0.9655\n",
      "[05] train loss=0.0867 acc=0.9742 | test acc=0.9696\n",
      "[06] train loss=0.0721 acc=0.9784 | test acc=0.9710\n",
      "[07] train loss=0.0585 acc=0.9825 | test acc=0.9732\n",
      "[08] train loss=0.0500 acc=0.9847 | test acc=0.9722\n",
      "[09] train loss=0.0414 acc=0.9877 | test acc=0.9761\n",
      "[10] train loss=0.0325 acc=0.9904 | test acc=0.9761\n",
      "[11] train loss=0.0272 acc=0.9915 | test acc=0.9758\n",
      "[12] train loss=0.0214 acc=0.9939 | test acc=0.9758\n",
      "[13] train loss=0.0174 acc=0.9949 | test acc=0.9759\n",
      "[14] train loss=0.0160 acc=0.9952 | test acc=0.9744\n",
      "[15] train loss=0.0110 acc=0.9972 | test acc=0.9791\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ok = tot = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        ok += (logits.argmax(1) == y).sum().item()\n",
    "        tot += y.size(0)\n",
    "    return ok / tot\n",
    "\n",
    "model = DeepClassifier(C=10, D=9).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, 16):\n",
    "    model.train()\n",
    "    loss_sum = acc_sum = n_sum = 0\n",
    "    for x, y in train_ld:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            acc_sum += (pred == y).sum().item()\n",
    "            n_sum += y.size(0)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "\n",
    "    tr_acc = acc_sum / n_sum\n",
    "    te_acc = evaluate(model, test_ld)\n",
    "    print(f\"[{epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect a Small Random Subset of Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_list, y_list = [], []\n",
    "max_points = int(len(train_ld.dataset) * 0.05)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in train_ld:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        z = model.encoder(X)\n",
    "        emb_list.append(z)\n",
    "        y_list.append(y)\n",
    "        if sum(t.shape[0] for t in emb_list) >= max_points:\n",
    "            break\n",
    "\n",
    "Z = torch.cat(emb_list, dim=0).cpu().numpy()[:max_points]\n",
    "Y = torch.cat(y_list, dim=0).cpu().numpy()[:max_points]\n",
    "Z.shape, Y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Learned Classifier Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = model.head.linear.weight.detach().cpu()\n",
    "b = model.head.linear.bias.detach().cpu()\n",
    "W.shape, b.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 6))\n",
    "cmap = plt.get_cmap(\"tab10\", 10)\n",
    "markers = ['o','s','^','v','<','>','P','X','D','*']\n",
    "\n",
    "for c in range(10):\n",
    "    idx = (Y == c)\n",
    "    plt.scatter(Z[idx, 0], Z[idx, 1], s=8, alpha=0.6,\n",
    "                c=[cmap(c)], marker=markers[c], label=f\"{c}\")\n",
    "\n",
    "plt.title(\"Deep softmax head on MNIST: sampled training embeddings\")\n",
    "plt.xlabel(\"Embedding dim 1\")\n",
    "plt.ylabel(\"Embedding dim 2\")\n",
    "plt.legend(fontsize=8, ncol=2, frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
