{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CIFAR-100 Classification with a Softmax Head\n",
        "We train a convolutional encoder with a softmax classifier on CIFAR-100 and visualise the learned embedding space.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device = cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device =', device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 10000)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean = (0.5071, 0.4867, 0.4408)\n",
        "std = (0.2675, 0.2565, 0.2761)\n",
        "pin_memory = torch.cuda.is_available()\n",
        "\n",
        "train_tfm = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "test_tfm = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "train_ds = datasets.CIFAR100(root='./data', train=True, transform=train_tfm, download=True)\n",
        "test_ds  = datasets.CIFAR100(root='./data', train=False, transform=test_tfm, download=True)\n",
        "train_ld = DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=4, pin_memory=pin_memory)\n",
        "test_ld  = DataLoader(test_ds,  batch_size=1024, shuffle=False, num_workers=4, pin_memory=pin_memory)\n",
        "len(train_ds), len(test_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model: encoder + softmax head\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "        )\n",
        "        self.proj = nn.Linear(256, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.proj(x)\n",
        "\n",
        "class SoftmaxHead(nn.Module):\n",
        "    def __init__(self, D, C):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(D, C)\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.linear(z)\n",
        "\n",
        "class DeepClassifier(nn.Module):\n",
        "    def __init__(self, C, D):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(D)\n",
        "        self.head = SoftmaxHead(D, C)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        return self.head(z)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train & Eval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[01] train loss=3.7270 acc=0.1230 | test acc=0.1416\n",
            "[02] train loss=3.0125 acc=0.2364 | test acc=0.2482\n",
            "[03] train loss=2.6081 acc=0.3183 | test acc=0.2846\n",
            "[04] train loss=2.3123 acc=0.3806 | test acc=0.3868\n",
            "[05] train loss=2.0999 acc=0.4279 | test acc=0.3972\n",
            "[06] train loss=1.9189 acc=0.4717 | test acc=0.3733\n",
            "[07] train loss=1.7778 acc=0.5050 | test acc=0.4510\n",
            "[08] train loss=1.6700 acc=0.5308 | test acc=0.4807\n",
            "[09] train loss=1.5865 acc=0.5500 | test acc=0.4999\n",
            "[10] train loss=1.4963 acc=0.5723 | test acc=0.5152\n",
            "[11] train loss=1.4222 acc=0.5935 | test acc=0.5227\n",
            "[12] train loss=1.3620 acc=0.6074 | test acc=0.5226\n",
            "[13] train loss=1.3089 acc=0.6212 | test acc=0.5374\n",
            "[14] train loss=1.2551 acc=0.6342 | test acc=0.5469\n",
            "[15] train loss=1.1938 acc=0.6498 | test acc=0.5390\n",
            "[16] train loss=1.1501 acc=0.6623 | test acc=0.5521\n",
            "[17] train loss=1.1125 acc=0.6711 | test acc=0.5708\n",
            "[18] train loss=1.0722 acc=0.6833 | test acc=0.5440\n",
            "[19] train loss=1.0262 acc=0.6935 | test acc=0.5741\n",
            "[20] train loss=0.9831 acc=0.7073 | test acc=0.5586\n",
            "[21] train loss=0.9529 acc=0.7131 | test acc=0.5938\n",
            "[22] train loss=0.9180 acc=0.7235 | test acc=0.5795\n",
            "[23] train loss=0.8830 acc=0.7338 | test acc=0.5897\n",
            "[24] train loss=0.8559 acc=0.7423 | test acc=0.6091\n",
            "[25] train loss=0.8213 acc=0.7495 | test acc=0.5957\n",
            "[26] train loss=0.7996 acc=0.7568 | test acc=0.6044\n",
            "[27] train loss=0.7784 acc=0.7601 | test acc=0.5993\n",
            "[28] train loss=0.7450 acc=0.7696 | test acc=0.6021\n",
            "[29] train loss=0.7196 acc=0.7774 | test acc=0.5886\n",
            "[30] train loss=0.6967 acc=0.7828 | test acc=0.6003\n",
            "[31] train loss=0.6747 acc=0.7874 | test acc=0.5968\n",
            "[32] train loss=0.6463 acc=0.7967 | test acc=0.6160\n",
            "[33] train loss=0.6251 acc=0.8043 | test acc=0.5733\n",
            "[34] train loss=0.6024 acc=0.8107 | test acc=0.6051\n",
            "[35] train loss=0.5838 acc=0.8154 | test acc=0.6068\n",
            "[36] train loss=0.5742 acc=0.8167 | test acc=0.6012\n",
            "[37] train loss=0.5441 acc=0.8261 | test acc=0.6148\n",
            "[38] train loss=0.5336 acc=0.8297 | test acc=0.6074\n",
            "[39] train loss=0.5122 acc=0.8363 | test acc=0.6038\n",
            "[40] train loss=0.4965 acc=0.8413 | test acc=0.5998\n",
            "[41] train loss=0.4776 acc=0.8457 | test acc=0.6115\n",
            "[42] train loss=0.4620 acc=0.8512 | test acc=0.6198\n",
            "[43] train loss=0.4471 acc=0.8554 | test acc=0.6267\n",
            "[44] train loss=0.4363 acc=0.8580 | test acc=0.6304\n",
            "[45] train loss=0.4198 acc=0.8629 | test acc=0.6308\n",
            "[46] train loss=0.4083 acc=0.8664 | test acc=0.6276\n",
            "[47] train loss=0.3938 acc=0.8705 | test acc=0.6276\n",
            "[48] train loss=0.3722 acc=0.8775 | test acc=0.6253\n",
            "[49] train loss=0.3701 acc=0.8783 | test acc=0.6140\n",
            "[50] train loss=0.3630 acc=0.8821 | test acc=0.6278\n",
            "[51] train loss=0.3537 acc=0.8827 | test acc=0.6274\n",
            "[52] train loss=0.3434 acc=0.8865 | test acc=0.6266\n",
            "[53] train loss=0.3368 acc=0.8883 | test acc=0.6351\n",
            "[54] train loss=0.3200 acc=0.8950 | test acc=0.6303\n",
            "[55] train loss=0.3192 acc=0.8943 | test acc=0.6330\n",
            "[56] train loss=0.3043 acc=0.8988 | test acc=0.6234\n",
            "[57] train loss=0.2955 acc=0.9010 | test acc=0.6350\n",
            "[58] train loss=0.2896 acc=0.9049 | test acc=0.6291\n",
            "[59] train loss=0.2775 acc=0.9085 | test acc=0.6186\n",
            "[60] train loss=0.2684 acc=0.9104 | test acc=0.6239\n",
            "[61] train loss=0.2693 acc=0.9106 | test acc=0.6175\n",
            "[62] train loss=0.2614 acc=0.9136 | test acc=0.6212\n",
            "[63] train loss=0.2520 acc=0.9166 | test acc=0.6365\n",
            "[64] train loss=0.2521 acc=0.9145 | test acc=0.6329\n",
            "[65] train loss=0.2526 acc=0.9146 | test acc=0.6338\n",
            "[66] train loss=0.2405 acc=0.9208 | test acc=0.6338\n",
            "[67] train loss=0.2270 acc=0.9255 | test acc=0.6328\n",
            "[68] train loss=0.2241 acc=0.9255 | test acc=0.6293\n",
            "[69] train loss=0.2293 acc=0.9232 | test acc=0.6340\n",
            "[70] train loss=0.2118 acc=0.9284 | test acc=0.6334\n",
            "[71] train loss=0.2247 acc=0.9245 | test acc=0.6234\n",
            "[72] train loss=0.2096 acc=0.9294 | test acc=0.6305\n",
            "[73] train loss=0.1975 acc=0.9345 | test acc=0.6293\n",
            "[74] train loss=0.2084 acc=0.9300 | test acc=0.6324\n",
            "[75] train loss=0.1926 acc=0.9358 | test acc=0.6309\n",
            "[76] train loss=0.1999 acc=0.9330 | test acc=0.6215\n",
            "[77] train loss=0.1920 acc=0.9359 | test acc=0.6352\n",
            "[78] train loss=0.1814 acc=0.9392 | test acc=0.6341\n",
            "[79] train loss=0.1798 acc=0.9404 | test acc=0.6291\n",
            "[80] train loss=0.1750 acc=0.9415 | test acc=0.6405\n",
            "[81] train loss=0.1705 acc=0.9426 | test acc=0.6284\n",
            "[82] train loss=0.1693 acc=0.9434 | test acc=0.6455\n",
            "[83] train loss=0.1790 acc=0.9398 | test acc=0.6371\n",
            "[84] train loss=0.1660 acc=0.9442 | test acc=0.6303\n",
            "[85] train loss=0.1582 acc=0.9479 | test acc=0.6360\n",
            "[86] train loss=0.1630 acc=0.9449 | test acc=0.6307\n",
            "[87] train loss=0.1632 acc=0.9445 | test acc=0.6303\n",
            "[88] train loss=0.1640 acc=0.9444 | test acc=0.6305\n",
            "[89] train loss=0.1579 acc=0.9474 | test acc=0.6331\n",
            "[90] train loss=0.1559 acc=0.9479 | test acc=0.6374\n",
            "[91] train loss=0.1455 acc=0.9509 | test acc=0.6302\n",
            "[92] train loss=0.1513 acc=0.9489 | test acc=0.6339\n",
            "[93] train loss=0.1496 acc=0.9499 | test acc=0.6291\n",
            "[94] train loss=0.1447 acc=0.9511 | test acc=0.6363\n",
            "[95] train loss=0.1397 acc=0.9536 | test acc=0.6432\n",
            "[96] train loss=0.1400 acc=0.9534 | test acc=0.6386\n",
            "[97] train loss=0.1348 acc=0.9551 | test acc=0.6404\n",
            "[98] train loss=0.1397 acc=0.9529 | test acc=0.6238\n",
            "[99] train loss=0.1418 acc=0.9535 | test acc=0.6527\n",
            "[100] train loss=0.1411 acc=0.9525 | test acc=0.6477\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    ok = tot = 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        ok += (logits.argmax(1) == y).sum().item()\n",
        "        tot += y.size(0)\n",
        "    return ok / tot\n",
        "\n",
        "model = DeepClassifier(C=100, D=99).to(device)\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, 101):\n",
        "    model.train()\n",
        "    loss_sum = acc_sum = n_sum = 0\n",
        "    for x, y in train_ld:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = loss_fn(logits, y)\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = logits.argmax(1)\n",
        "            acc_sum += (pred == y).sum().item()\n",
        "            n_sum += y.size(0)\n",
        "            loss_sum += loss.item() * y.size(0)\n",
        "\n",
        "    tr_acc = acc_sum / n_sum\n",
        "    te_acc = evaluate(model, test_ld)\n",
        "    print(f\"[{epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "5828cde9504051a95539f11701fe4fe633331e3e80ce955880f142f23888e20c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
