{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CIFAR-100 Classification with a Softmax Head\n",
        "We train a convolutional encoder with a softmax classifier on CIFAR-100 and visualise the learned embedding space.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device = cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device =', device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 10000)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean = (0.5071, 0.4867, 0.4408)\n",
        "std = (0.2675, 0.2565, 0.2761)\n",
        "pin_memory = torch.cuda.is_available()\n",
        "\n",
        "train_tfm = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "test_tfm = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "train_ds = datasets.CIFAR100(root='./data', train=True, transform=train_tfm, download=True)\n",
        "test_ds  = datasets.CIFAR100(root='./data', train=False, transform=test_tfm, download=True)\n",
        "train_ld = DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=4, pin_memory=pin_memory)\n",
        "test_ld  = DataLoader(test_ds,  batch_size=1024, shuffle=False, num_workers=4, pin_memory=pin_memory)\n",
        "len(train_ds), len(test_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model: encoder + softmax head\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "        )\n",
        "        self.proj = nn.Linear(256, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.proj(x)\n",
        "\n",
        "class SoftmaxHead(nn.Module):\n",
        "    def __init__(self, D, C):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(D, C)\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.linear(z)\n",
        "\n",
        "class DeepClassifier(nn.Module):\n",
        "    def __init__(self, C, D):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(D)\n",
        "        self.head = SoftmaxHead(D, C)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        return self.head(z)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train & Eval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[01] train loss=3.7252 acc=0.1193 | test acc=0.1368\n",
            "[02] train loss=2.9818 acc=0.2437 | test acc=0.2477\n",
            "[03] train loss=2.5779 acc=0.3259 | test acc=0.2934\n",
            "[04] train loss=2.3148 acc=0.3809 | test acc=0.2978\n",
            "[05] train loss=2.1049 acc=0.4306 | test acc=0.3715\n",
            "[06] train loss=1.9226 acc=0.4718 | test acc=0.4214\n",
            "[07] train loss=1.7853 acc=0.5035 | test acc=0.4831\n",
            "[08] train loss=1.6781 acc=0.5309 | test acc=0.4376\n",
            "[09] train loss=1.5726 acc=0.5568 | test acc=0.4694\n",
            "[10] train loss=1.4927 acc=0.5795 | test acc=0.5061\n",
            "[11] train loss=1.4206 acc=0.5966 | test acc=0.5123\n",
            "[12] train loss=1.3570 acc=0.6084 | test acc=0.5310\n",
            "[13] train loss=1.2945 acc=0.6278 | test acc=0.4844\n",
            "[14] train loss=1.2395 acc=0.6411 | test acc=0.5461\n",
            "[15] train loss=1.1928 acc=0.6539 | test acc=0.5631\n",
            "[16] train loss=1.1370 acc=0.6672 | test acc=0.5652\n",
            "[17] train loss=1.0933 acc=0.6781 | test acc=0.5654\n",
            "[18] train loss=1.0510 acc=0.6900 | test acc=0.5922\n",
            "[19] train loss=1.0106 acc=0.6987 | test acc=0.5904\n",
            "[20] train loss=0.9742 acc=0.7062 | test acc=0.5992\n",
            "[21] train loss=0.9423 acc=0.7171 | test acc=0.5925\n",
            "[22] train loss=0.9056 acc=0.7282 | test acc=0.5714\n",
            "[23] train loss=0.8686 acc=0.7394 | test acc=0.5806\n",
            "[24] train loss=0.8483 acc=0.7418 | test acc=0.5716\n",
            "[25] train loss=0.8130 acc=0.7534 | test acc=0.5908\n",
            "[26] train loss=0.7881 acc=0.7571 | test acc=0.5915\n",
            "[27] train loss=0.7621 acc=0.7649 | test acc=0.6173\n",
            "[28] train loss=0.7339 acc=0.7737 | test acc=0.5971\n",
            "[29] train loss=0.7114 acc=0.7808 | test acc=0.6145\n",
            "[30] train loss=0.6816 acc=0.7871 | test acc=0.6110\n",
            "[31] train loss=0.6534 acc=0.7966 | test acc=0.6085\n",
            "[32] train loss=0.6345 acc=0.8021 | test acc=0.6165\n",
            "[33] train loss=0.6134 acc=0.8073 | test acc=0.6074\n",
            "[34] train loss=0.5960 acc=0.8108 | test acc=0.5900\n",
            "[35] train loss=0.5707 acc=0.8208 | test acc=0.6058\n",
            "[36] train loss=0.5560 acc=0.8246 | test acc=0.6097\n",
            "[37] train loss=0.5388 acc=0.8303 | test acc=0.6319\n",
            "[38] train loss=0.5157 acc=0.8352 | test acc=0.6230\n",
            "[39] train loss=0.5006 acc=0.8385 | test acc=0.6095\n",
            "[40] train loss=0.4853 acc=0.8452 | test acc=0.6259\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    ok = tot = 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        ok += (logits.argmax(1) == y).sum().item()\n",
        "        tot += y.size(0)\n",
        "    return ok / tot\n",
        "\n",
        "model = DeepClassifier(C=100, D=99).to(device)\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, 41):\n",
        "    model.train()\n",
        "    loss_sum = acc_sum = n_sum = 0\n",
        "    for x, y in train_ld:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = loss_fn(logits, y)\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = logits.argmax(1)\n",
        "            acc_sum += (pred == y).sum().item()\n",
        "            n_sum += y.size(0)\n",
        "            loss_sum += loss.item() * y.size(0)\n",
        "\n",
        "    tr_acc = acc_sum / n_sum\n",
        "    te_acc = evaluate(model, test_ld)\n",
        "    print(f\"[{epoch:02d}] train loss={loss_sum/n_sum:.4f} acc={tr_acc:.4f} | test acc={te_acc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "5828cde9504051a95539f11701fe4fe633331e3e80ce955880f142f23888e20c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
